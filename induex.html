<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">

	<title>Artificial Inteligent</title>
	<link rel="stylesheet" type="text/css" href="style.css">
</head>
<body>
     <div class="topnav">
        <a class="active" href="">Home</a>

    
        <a href="https://www.techtarget.com/searchenterpriseai/definition/AI-Artificial-Intelligence">About</a>
      </div>
        <center>
        <br>
        <br>
    <H1>Artificial Inteligent</H1>
    <img src="111.jpg" alt="Jack Ma" width="700">
    <hr>
    </center>

    <br>
    <h2>Father of AI</h2>
    <p>John McCarthy is considered as the father of Artificial Intelligence. John McCarthy was an American computer scientist. The term "artificial intelligence" was coined by him.The conceptualization and progressive development of AI was started in 1940s; however, it was John McCarthy, a Stanford University researcher, who first coined this term. John McCarthy is popular as the father of Artificial Intelligence.Artificial Intelligence or simply AI is an experimental science being developed with the purpose to understand the nature of intelligent thought and subsequent action. It is presented by machines or software (computer).In today’s context, largely, but of course not exclusively, Artificial Intelligence is related to Computer.Therefore, study of AI also involves other disciplines including Psychology, Philosophy, Science, etc</p>
<p>Birth of AI: 1950-1956
Dates of note: 1950: Alan Turing published “Computer Machinery and Intelligence” which proposed a test of machine intelligence called The Imitation Game. 1952: A computer scientist named Arthur Samuel developed a program to play checkers, which is the first to ever learn the game independently.What is the Right Age to Start AI Learning? Several studies have proved that young minds are quicker learners than any other age of life. So, we can consider the least possible age i.e. 2-3 years of age will be perfect to start exploring AI.The period between 1940 and 1960 was strongly marked by the conjunction of technological developments (of which the Second World War was an accelerator) and the desire to understand how to bring together the functioning of machines and organic beings. For Norbert Wiener, a pioneer in cybernetics, the aim was to unify mathematical theory, electronics and automation as "a whole theory of control and communication, both in animals and machines".</p>

<p>In 1968 Stanley Kubrick directed the film "2001 Space Odyssey" where a computer - HAL 9000 (only one letter away from those of IBM) summarizes in itself the whole sum of ethical questions posed by AI: will it represent a high level of sophistication, a good for humanity or a danger? The impact of the film will naturally not be scientific but it will contribute to popularize the theme, just as the science fiction author Philip K. Dick, who will never cease to wonder if, one day, the machines will experience emotions.It was with the advent of the first microprocessors at the end of 1970 that AI took off again and entered the golden age of expert systems.The path was actually opened at MIT in 1965 with DENDRAL (expert system specialized in molecular chemistry) and at Stanford University in 1972 with MYCIN (system specialized in the diagnosis of blood diseases and prescription drugs). These systems were based on an "inference engine," which was programmed to be a logical mirror of human reasoning. By entering data, the engine provided answers of a high level of expertise.The promises foresaw a massive development but the craze will fall again at the end of 1980, early 1990. The programming of such knowledge actually required a lot of effort and from 200 to 300 rules, there was a "black box" effect where it was not clear how the machine reasoned. Development and maintenance thus became extremely problematic and - above all - faster and in many other less complex and less expensive ways were possible. It should be recalled that in the 1990s, the term artificial intelligence had almost become taboo and more modest variations had even entered university language, such as "advanced computing".</p>
  
  <hr>
    <center><img src="222.jpg" alt="Jack ma" width="700"></center>
    <h2>Artificiale Inteligent Benifit  For Human</h2>
   




    <p>
    	<ul>
    		<li>Automation</li>
    		<li>Smart Decision Making</li>
    		<li>Enhanced Customer Experience</li>
   	         <li> Medical Advances</li>
   	         <li>Research and Data Analysis</li>
   	         <li>Solving Complex Problems
</li>
   	         <li>Business Continuity</li>
   	         <li>Managing Repetitive Tasks</li>
   	         <li>Minimizing Errors</li>
   	         <li>Increased Business Efficiency</li>

    	</ul>
    </p>
<p>Artificial Intelligence has been around for a great deal of time now. The benefits of AI gradually improving our everyday life. The technology is being used for robots that greet at shopping centers or online search engines for offering suggestions.AI simulates human reasoning in artificial intelligence systems. It is the ability of the computer program to think and learn. Everything can be taken to be AI if it involves a program that does something that we usually think depends on human intelligence.Innovations in the Artificial Intelligence space have led to several benefits across multiple industries. Processes are effective and efficient, convenient technologies are extensively available, and forecasts are more accurate.In this guide, we are going to walk you through the benefits of AI.</p>

<p>Artificial Intelligence can be extremely beneficial in the education sector. The technology helps with the development and setup of many learning programs. It can also be used to develop games and software programs. With Artificial Intelligence, it is possible to redesign and reform the whole education system and techniques of teaching. It begins by issuing certificates and degrees in schools and colleges.Not only institutions but also students can benefit from AI-based applications. By using them in the education field, you have the potential to change the teaching and learning process. This helps in improving the whole process. it improvises and changes learning activities for making all students better learners.Artificial Intelligence caters to the requirements of students having special needs..</p>
<br>
<hr>
<center><img src="333.jpg" alt="Jack Ma" width="700" ></center>
<h2>Future in AI</h2>
<p>AI’s influence on technology is due in part because of how it impacts computing. Through AI, computers have the ability to harness massive amounts of data and use their learned intelligence to make optimal decisions and discoveries in fractions of the time that it would take humans.

AI has come a long way since 1951, when the first documented success of an AI computer program was written by Christopher Strachey, whose checkers program completed a whole game on the Ferranti Mark I computer at the University of Manchester.

Since then, AI has been used to help sequence RNA for vaccines and model human speech, technologies that rely on model- and algorithm-based machine learning and increasingly focus on perception, reasoning and generalization. With innovations like these, AI has re-taken center stage like never before — and it won’t cede the spotlight anytime soon. </p>

<p>There’s virtually no major industry that modern AI — more specifically, “narrow AI,” which performs objective functions using data-trained models and often falls into the categories of deep learning or machine learning — hasn’t already affected. That’s especially true in the past few years, as data collection and analysis has ramped up considerably thanks to robust IoT connectivity, the proliferation of connected devices and ever-speedier computer processing.

“I think anybody making assumptions about the capabilities of intelligent software capping out at some point are mistaken,” David Vandegrift, CTO and co-founder of the customer relationship management firm 4Degrees, said.

With companies spending billions of dollars on AI products and services annually, tech giants like Google, Apple, Microsoft and Amazon spending billions to create those products and services, universities making AI a more prominent part of their curricula and the U.S. Department of Defense upping its AI game, big things are bound to happen. 

“Lots of industries go through this pattern of winter, winter, and then an eternal spring,” former Google Brain leader and Baidu chief scientist Andrew Ng told ZDNet. “We may be in the eternal spring of AI.”

Some sectors are at the start of their AI journey, others are veteran travelers. Both have a long way to go. Regardless, the impact AI is having on our present day lives is hard to ignore.

 </p>



<br>
<hr>
<center><img src="444.jpg" alt="Jack Ma" width="700"></center>
<h2></h2>






</body>
</html>